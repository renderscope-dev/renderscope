{
  "id": "pytorch3d",
  "name": "PyTorch3D",
  "version": "0.7.8",
  "description": "Meta's modular differentiable rendering and 3D deep learning library with native PyTorch integration",
  "long_description": "PyTorch3D is a library developed by Meta's Fundamental AI Research (FAIR) team that provides efficient, reusable components for 3D computer vision research within the PyTorch ecosystem. At its core is a differentiable mesh rasterizer and point cloud renderer designed for integration into gradient-based training pipelines, enabling tasks such as 3D reconstruction from images, novel view synthesis, shape prediction, and texture estimation.\n\nThe library's architecture is built around batched operations — it can render hundreds of meshes or point clouds simultaneously in a single forward pass, which is critical for efficient training with mini-batches. Beyond rendering, PyTorch3D provides a comprehensive set of differentiable 3D operators: mesh I/O and manipulation, rigid and non-rigid transforms, surface sampling, Chamfer distance and Earth Mover's distance loss functions, and a volumetric renderer for NeRF-style implicit representations.\n\nPyTorch3D's renderer uses simplified shading models (Phong, flat, textured) rather than physically based light transport, reflecting its design goal of fast gradient computation at training resolutions rather than photorealistic output. The C++/CUDA backend provides GPU-accelerated implementations of all core operations, while the Python API offers the familiar PyTorch tensor interface. The library has become one of the most widely cited tools in 3D deep learning research, with adoption across thousands of papers and projects in the computer vision community.",
  "technique": ["differentiable", "rasterization"],
  "language": "Python/C++/CUDA",
  "license": "BSD-3-Clause",
  "platforms": ["linux", "macos", "windows"],
  "gpu_support": true,
  "gpu_apis": ["cuda"],
  "cpu_support": true,
  "real_time": false,
  "scene_formats": ["obj", "ply", "gltf", "programmatic"],
  "output_formats": ["tensor", "png"],
  "homepage": "https://pytorch3d.org",
  "repository": "https://github.com/facebookresearch/pytorch3d",
  "documentation": "https://pytorch3d.readthedocs.io/en/latest/",
  "paper": "https://arxiv.org/abs/2007.08501",
  "paper_bibtex": "@inproceedings{ravi2020pytorch3d,\n  title={Accelerating 3D Deep Learning with PyTorch3D},\n  author={Ravi, Nikhila and Reizenstein, Jeremy and Novotny, David and Gordon, Taylor and Lo, Wan-Yen and Johnson, Justin and Gkioxari, Georgia},\n  booktitle={SIGGRAPH Asia 2020 Courses},\n  year={2020},\n  publisher={ACM}\n}",
  "first_release": "2020-01-23",
  "latest_release": "2024-08-15",
  "latest_release_version": "0.7.8",
  "status": "active",
  "github_stars": 8800,
  "github_stars_trend": [7400, 7600, 7800, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800],
  "commit_activity_52w": [5, 3, 8, 12, 7, 4, 2, 0, 6, 9, 11, 3, 5, 7, 4, 2, 1, 8, 6, 10, 14, 5, 3, 2, 7, 6, 4, 3, 9, 8, 5, 2, 1, 4, 6, 7, 3, 5, 8, 11, 6, 4, 2, 3, 5, 7, 9, 4, 3, 6, 8, 5],
  "contributor_count": 120,
  "open_issues": 250,
  "fork_count": 1350,
  "tags": [
    "differentiable",
    "pytorch",
    "rasterization",
    "mesh-rendering",
    "point-cloud",
    "inverse-rendering",
    "meta",
    "cuda",
    "3d-deep-learning",
    "batched-rendering"
  ],
  "strengths": [
    "Native PyTorch tensor integration — rendering inputs and outputs are standard PyTorch tensors, enabling seamless end-to-end gradient flow between 3D rendering and neural network layers without format conversion or memory copies",
    "First-class batched rendering support processes hundreds of meshes and point clouds in parallel within a single forward pass, essential for efficient mini-batch training in 3D deep learning pipelines",
    "Comprehensive differentiable 3D operator library beyond rendering: mesh sampling, Chamfer distance, IoU loss, rigid transforms, and texture mapping are all differentiable, reducing dependence on external packages",
    "Includes both a mesh rasterizer and a volumetric renderer for NeRF-style implicit representations in a unified API, covering the two dominant paradigms in neural 3D representation learning",
    "Backed by Meta FAIR with consistent releases, extensive documentation, and widespread adoption — cited in over 1,500 research papers as of 2024"
  ],
  "limitations": [
    "Limited to simplified shading models (Phong, flat, textured) — no global illumination, physically based light transport, or advanced material models, which restricts visual fidelity of rendered outputs",
    "CUDA is effectively required for any practical workload — CPU fallbacks exist but are orders of magnitude slower, making development and debugging on non-NVIDIA hardware impractical",
    "Installation is sensitive to the exact combination of PyTorch version, CUDA toolkit version, and compiler toolchain — version mismatches produce cryptic build failures that are difficult to diagnose",
    "Not designed for photorealistic image production — the renderer targets gradient computation at training resolutions (typically 256×256), not final-quality visual output"
  ],
  "best_for": "Researchers integrating 3D mesh or point cloud rendering into PyTorch training pipelines for reconstruction, view synthesis, shape prediction, or texture estimation tasks",
  "not_ideal_for": "Producing photorealistic images or any workflow that does not involve gradient-based optimization — classical path tracers like PBRT or Cycles are better suited for visual quality",
  "related": ["nvdiffrast", "nvidia-kaolin", "redner", "drjit", "nerfstudio", "3d-gaussian-splatting", "mitsuba3"],
  "features": {
    "global_illumination": false,
    "path_tracing": false,
    "bidirectional_pt": false,
    "metropolis_lt": false,
    "photon_mapping": false,
    "volumetric": true,
    "subsurface_scattering": false,
    "motion_blur": false,
    "depth_of_field": false,
    "spectral_rendering": false,
    "polarization": false,
    "caustics": false,
    "instancing": false,
    "out_of_core": false,
    "adaptive_sampling": false,
    "denoiser_builtin": false,
    "gpu_rendering": true,
    "multi_gpu": false,
    "network_rendering": false,
    "hardware_ray_tracing": false,
    "differentiable": true,
    "inverse_rendering": true,
    "neural_acceleration": false,
    "real_time_preview": false,
    "pbr_materials": false,
    "hdri_environment": false,
    "shadow_mapping": null,
    "screen_space_effects": null,
    "lod_system": false,
    "animation_support": false,
    "python_api": true,
    "c_cpp_api": false,
    "rust_api": false,
    "javascript_api": false,
    "scene_editor_gui": false,
    "plugin_system": false,
    "open_shading_language": false,
    "materialx": false
  },
  "integrations": ["PyTorch"],
  "install_command": "pip install pytorch3d",
  "community_links": {
    "forum": "https://github.com/facebookresearch/pytorch3d/discussions"
  },
  "citations": [
    {
      "title": "Accelerating 3D Deep Learning with PyTorch3D",
      "url": "https://arxiv.org/abs/2007.08501",
      "year": 2020
    }
  ],
  "tutorials": [
    {
      "title": "PyTorch3D Tutorials",
      "url": "https://pytorch3d.org/tutorials",
      "type": "article"
    },
    {
      "title": "Deform a Source Mesh to a Target Mesh",
      "url": "https://pytorch3d.org/tutorials/deform_source_mesh_to_target_mesh",
      "type": "article"
    }
  ]
}